{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 library 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install beautifulsoup4\n",
    "!pip install pickle-mixin\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 크롤링 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selenium 불러오기\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options to look like a human\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\") #관리자 모드에서 접근을 할 때 필요\n",
    "\n",
    "#기계라고 생각되면 접속을 차단할 수도 있음 따라서 옵션을 줌\n",
    "options.add_argument(\"window-size=1920x1080\") \n",
    "options.add_argument(\"lang=ko_KR\")\n",
    "options.add_argument(\"user-agent=Chrome/89.0.4389.114\")\n",
    "\n",
    "# to save error log\n",
    "service_args = ['--verbose']   \n",
    "service_log_path = \"./chromedriver.log\"  #에러가 났을 때 log 찍을 수 있게 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chrome창 열기\n",
    "driver = webdriver.Chrome(executable_path =\"./chromedriver\",\n",
    "                         options = options,\n",
    "                         service_args = service_args,\n",
    "                         service_log_path = service_log_path)"
   ]
  },
  {
   "source": [
    "## 키워드를 통해 검색  \n",
    "+ 브런치가 정한 키워드와 유저가 직접 키워드를 지정해 검색하는 방법을 나눈다.\n",
    "+ 브런치는 스크롤을 통해 글들을 불러오는 형식이므로 스크롤을 자동으로 내려주는 코드를 추가한다.\n",
    "+ 키워드에 해당하는 글을 가져오기보다 글들의 url을 저장한다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brunch_url_keyword(keyword, user_selected = True):\n",
    "    if user_selected:\n",
    "        url = \"https://brunch.co.kr/search?q=\"+ keyword\n",
    "    else:\n",
    "        url = \"https://brunch.co.kr/keyword/\" + keyword + \"?q=g\"\n",
    "    driver.get(url)\n",
    "\n",
    "    # 스크롤 높이\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\") \n",
    "\n",
    "\n",
    "    for i in range(1000): \n",
    "        # 스크롤 무빙 \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "        # 페이지 로드 대기 \n",
    "        time.sleep(3) \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\")\n",
    "        time.sleep(3) \n",
    "    \n",
    "        # 새로운 스크롤 동작 높이와 이전 스크롤과의 비교 \n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\") \n",
    "        if new_height == last_height: \n",
    "            break \n",
    "    \n",
    "        last_height = new_height\n",
    "\n",
    "    source = driver.page_source\n",
    "    data = source.encode('utf-8')\n",
    "    bs = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    urls = bs.select('#wrapArticle > div.wrap_article_list.\\#keyword_related_contents > ul > li')\n",
    "    print(len(urls))\n",
    "\n",
    "    # 파일로 저장\n",
    "    filename = keyword + \"_url.txt\"\n",
    "    f = open(filename, 'w')\n",
    "    for val in urls:\n",
    "        data = val + \"\\n\"\n",
    "        f.write(data)\n",
    "    f.close()\n",
    "\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 브런치가 정해놓은 키워드로 검색\n",
    "brunch_url_keyword(\"감성_에세이\",False)\n",
    "brunch_url_keyword(\"문화·예술\",False)\n",
    "brunch_url_keyword(\"취향저격_영화_리뷰\",False)\n",
    "brunch_url_keyword(\"사랑·이별\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저가 직접 키워드를 검색\n",
    "brunch_url_keyword(\"기쁨\")\n",
    "brunch_url_keyword(\"슬픔\")\n",
    "brunch_url_keyword(\"분노\")\n",
    "brunch_url_keyword(\"공포\")\n",
    "brunch_url_keyword(\"사랑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_url(keyword):\n",
    "    file_name = './브런치데이터/'+ keyword + \"_url.txt\"\n",
    "    b = []\n",
    "    f = open(file_name, 'r')\n",
    "    a = f.readlines()\n",
    "    for l in a:\n",
    "        before = l.replace('\\n', '')\n",
    "        b.append(before)\n",
    "    return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                            # u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\xa0\"\n",
    "                               u\"\\ucdee\"\n",
    "                               u'\\ude0a'\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', str(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rawText_req(url_list):\n",
    "    doc_df = pd.DataFrame(columns = ['text'])\n",
    "    for url in url_list:\n",
    "        \n",
    "        #각 url로 글에 접근\n",
    "        req = requests.get(url)\n",
    "        html = req.text\n",
    "        time.sleep(0.03)\n",
    "        data = html.encode('utf-8')\n",
    "        bs = BeautifulSoup(data, 'html.parser')\n",
    "        \n",
    "\n",
    "        #글 가져오기\n",
    "        doc = bs.select('body > div.service_contents.article_contents > div.wrap_view_article > div.wrap_body')\n",
    "        raw_doc = \"\"\n",
    "\n",
    "        if not doc:\n",
    "            continue\n",
    "        elif doc[0].select('h4') != []:\n",
    "            for d in doc[0].select('h4'):\n",
    "                par = d.get_text().replace(u'xa0', ' ').replace('&nbsp;',' ').replace(u'\\udd25', ' ').replace(u'\\ucdee', ' ')\n",
    "                par = remove_emoji(par)\n",
    "                par = re.compile('[^가-힣0-9ㄱ-ㅎㅏ-ㅣ\\.\\?\\!,^]+').sub(' ', par)\n",
    "                raw_doc = raw_doc + str(par)\n",
    "        elif doc[0].select('p') != []:\n",
    "            for d in doc[0].select('p'):\n",
    "                par = d.get_text().replace(u'xa0', ' ').replace('&nbsp;',' ').replace(u'\\udd25', ' ').replace(u'\\ucdee', ' ')\n",
    "                par = remove_emoji(par)\n",
    "                par = re.compile('[^가-힣0-9ㄱ-ㅎㅏ-ㅣ\\.\\?\\!,^]+').sub(' ', par)\n",
    "                raw_doc = raw_doc + str(par)\n",
    "    \n",
    "        #dataframe에 append\n",
    "        print(raw_doc + \"\\n\")\n",
    "        doc_df = doc_df.append({'text' : raw_doc}, ignore_index = True)\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    print(doc_df)\n",
    "\n",
    "    return doc_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "get_rawText_req(read_url('url_scary_keyword.txt')).to_excel('scary.xlsx')\n",
    "get_rawText_req(read_url('url_love_and_farewell.txt')).to_excel('love_farewell.xlsx')\n",
    "get_rawText_req(read_url('url_movie_review.txt')).to_excel('movie_review.xlsx')\n",
    "get_rawText_req(read_url('url_senti_essay.txt')).to_excel('senti_essay.xlsx')\n",
    "get_rawText_req(read_url('url_happy_keyword.txt')).to_excel('happy.xlsx')\n",
    "get_rawText_req(read_url('url_angry_keyword.txt')).to_excel('angry.xlsx')\n",
    "get_rawText_req(read_url('url_sad_keyword.txt')).to_excel('sad.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python376jvsc74a57bd08fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}